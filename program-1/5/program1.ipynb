{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize**\n",
    "\n",
    "Import packages and initialize train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "# read in the train dataset\n",
    "train_mat = pd.read_csv(\n",
    "    'train.dat', \n",
    "     sep='delimiter', header=None, engine ='python')\n",
    "\n",
    "# read in the test dataset\n",
    "test = pd.read_csv(\n",
    "    'test.dat', \n",
    "     sep='delimiter', header=None, engine ='python')\n",
    "\n",
    "# separate docs and classes\n",
    "train_vals = train_mat.iloc[:,:].values\n",
    "train_docs = [n[0][2:] for n in train_vals]\n",
    "train_cls = [n[0][0] for n in train_vals]\n",
    "\n",
    "# separate docs and classes\n",
    "test_vals = test.iloc[:,:].values\n",
    "test_docs = [n[0][0:] for n in test_vals]\n",
    "test_cls = [] * len(test_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Data Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(docs):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        words = word_tokenize(doc.lower())\n",
    "        words = [word for word in words if word not in stop_words and word not in punctuation]\n",
    "        cleaned_docs.append(\" \".join(words))\n",
    "    return cleaned_docs\n",
    "\n",
    "def stem_docs(docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_docs = []\n",
    "    for doc in docs:\n",
    "        stemmed_doc = ' '.join(stemmer.stem(word) for word in doc.split())\n",
    "        stemmed_docs.append(stemmed_doc)\n",
    "    return stemmed_docs\n",
    "\n",
    "def lemmatize_docs(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_docs = []\n",
    "    for doc in docs:\n",
    "        token_list = nltk.word_tokenize(doc)\n",
    "        pos_tags = nltk.pos_tag(token_list)\n",
    "        lemmatized_tokens = []\n",
    "        for token, pos in pos_tags:\n",
    "            pos_letter = pos[0].lower() if pos[0].lower() in ['a', 'n', 'v', 'r'] else 'n'\n",
    "            lemma = lemmatizer.lemmatize(token, pos=pos_letter)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "        lemmatized_docs.append(\" \".join(lemmatized_tokens))\n",
    "    return lemmatized_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Data**\n",
    "\n",
    "Perform stopword removal on train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to remove stopwords:  21.748326301574707\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_docs = remove_stopwords(train_docs)\n",
    "test_docs = remove_stopwords(test_docs)\n",
    "print(\"Time to remove stopwords: \", time.time() - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform lemmatization on train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to lemmatize:  212.67791604995728\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_docs = lemmatize_docs(train_docs)\n",
    "test_docs = lemmatize_docs(test_docs)\n",
    "print(\"Time to lemmatize: \", time.time() - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform stemming on train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# train_docs = stem_documents(train_docs)\n",
    "# test_docs = stem_documents(test_docs)\n",
    "# print(\"Time to stem: \", time.time() - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a subset of the train dataset to find best c, k values using f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs_subset = train_docs[0:2000]\n",
    "train_cls_subset = train_cls[0:2000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse Matrix Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmer(doc, c=3):\n",
    "    r\"\"\" Given a name and parameter c, return the vector of c-mers associated with the doc\n",
    "    \"\"\"\n",
    "    doc = doc.lower()\n",
    "    if len(doc) < c:\n",
    "        return [doc]\n",
    "    v = []\n",
    "    for i in range(len(doc)-c+1):\n",
    "        v.append(doc[i:(i+c)])\n",
    "    return v\n",
    "\n",
    "def build_matrix(docs, idx):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(list(w for w in d if w in idx))\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common() if k in idx)\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            if k in idx:\n",
    "                ind[j+n] = idx[k]\n",
    "                val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empty=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empty, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empty:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "    \n",
    "def build_idx(train_mat):\n",
    "    r\"\"\" Build a mapping from word to ID and vice versa. \n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    for d in train_mat:\n",
    "        for w in d:\n",
    "            w.lower()\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    return idx\n",
    "\n",
    "def textToMatrix(train_docs, test_docs, c=3):\n",
    "    train_mat = [cmer(l,c) for l in train_docs]\n",
    "    test_mat = [cmer(l,c) for l in test_docs]\n",
    "    return train_mat, test_mat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used for testing c, k values\n",
    "def split_data(mat, cls, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and cls into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( cls[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = cls[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste\n",
    "\n",
    "# my knn implementation\n",
    "def classify(test_row, train_mat, train_cls, k):\n",
    "    # calc similarities between test_row and training data\n",
    "    dots = test_row.dot(train_mat.T).todense().tolist()[0]\n",
    "    similarities = [dots[i] for i in range(len(dots))]\n",
    "\n",
    "    # if no neighbors were found, choose 1\n",
    "    if sum(similarities) == 0:\n",
    "        return random.randint(1, 4)\n",
    "\n",
    "    # get the indices of the k-nns\n",
    "    nn_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:k]\n",
    "\n",
    "    # count the occurrence of each class label among the nns\n",
    "    nn_labels = [train_cls[i] for i in nn_indices]\n",
    "    label_counts = Counter(nn_labels)\n",
    "\n",
    "    # if tie, perform a weighted vote based on similarity\n",
    "    if len(label_counts) > 1 and label_counts.most_common(1)[0][1] == label_counts.most_common(2)[1][1]:\n",
    "        label_weights = {label: 0 for label in label_counts.keys()}\n",
    "        for i in nn_indices:\n",
    "            label_weights[train_cls[i]] += similarities[i]\n",
    "        return max(label_weights, key=label_weights.get)\n",
    "\n",
    "    # else, return the majority label\n",
    "    return label_counts.most_common(1)[0][0]\n",
    "\n",
    "# only used for testing c, k values\n",
    "def classify_docs(train_docs, train_cls, c, k, d=10):\n",
    "    r\"\"\" Classify docs using c-mer frequency vector representations of the names and kNN classification with \n",
    "    cosine similarity and 10-fold cross validation\n",
    "    \"\"\"\n",
    "    docs = [cmer(n, c) for n in train_docs]\n",
    "    idx = build_idx(docs)\n",
    "    mat = build_matrix(docs, idx)\n",
    "    # since we're using cosine similarity, normalize the vectors\n",
    "    csr_l2normalize(mat)\n",
    "    \n",
    "    f1 = 0.0\n",
    "    accuracy = 0.0\n",
    "    macc = 0.0\n",
    "    for f in range(d):\n",
    "        # split data into training and testing\n",
    "        train, clstr, test, clste = split_data(mat, train_cls, f+1, d)\n",
    "        # predict the class of each test sample\n",
    "        clspr = [ classify(test[i,:], train, clstr, k) for i in range(test.shape[0]) ]\n",
    "        # compute the f1, accuracy of the prediction\n",
    "        acc = 0.0\n",
    "        for i in range(len(clste)):\n",
    "            if clste[i] == clspr[i]:\n",
    "                acc += 1\n",
    "        acc /= len(clste)\n",
    "        macc += acc\n",
    "        f1 += f1_score(clste, clspr, average='macro')\n",
    "        accuracy += accuracy_score(clste, clspr)\n",
    "        \n",
    "    return f1/d, accuracy/d, macc/d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Train Subset**\n",
    "\n",
    "Get best c, k values by calculating the f1_score of the train subset using c, k values 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = (0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "for c in range(1,10+1):\n",
    "    for k in range(1, 10+1):\n",
    "        f1, accuracy, macc = classify_docs(train_docs_subset, train_cls_subset, c=c, k=k)\n",
    "        #print(\"c=%d, k=%d, %f, %f, %f\" % (c, k, f1, accuracy, macc))\n",
    "        if f1 > best[2]:\n",
    "            best = (c, k, f1, accuracy, macc)\n",
    "#print(\"Best: c=%d, k=%d, %f, %f, %f\" % best)\n",
    "\n",
    "c = best[0]\n",
    "k = best[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Matrices**\n",
    "\n",
    "Build and normalize matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mat [nrows 102080, ncols 129391, nnz 15727513]\n",
      "test_mat [nrows 25520, ncols 129391, nnz 3922131]\n"
     ]
    }
   ],
   "source": [
    "train_mat, test_mat = textToMatrix(train_docs, test_docs, c)\n",
    "idx = build_idx(train_mat)\n",
    "\n",
    "train_mat = csr_l2normalize(build_matrix(train_mat, idx), copy=True)\n",
    "test_mat = csr_l2normalize(build_matrix(test_mat, idx), copy=True)\n",
    "\n",
    "csr_info(train_mat, \"train_mat\")\n",
    "csr_info(test_mat, \"test_mat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Predictions**\n",
    "\n",
    "Predict classifications on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cls = [ classify(test_mat[i,:], train_mat, train_cls, k) for i in range(test_mat.shape[0]) ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "\n",
    "Output predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('output.dat', 'w', newline='')\n",
    "predictions = pd.Series(test_cls)\n",
    "predictions.to_csv(output_file, index=False, header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output predictions to file (other version not always recording every row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yayaya.dat', 'w') as f:\n",
    "    for item in test_cls:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
